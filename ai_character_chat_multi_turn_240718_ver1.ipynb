{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86bcec4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T11:03:41.243332Z",
     "start_time": "2024-07-18T11:03:41.236671Z"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "535a53dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T11:03:41.985903Z",
     "start_time": "2024-07-18T11:03:41.950947Z"
    }
   },
   "outputs": [],
   "source": [
    "# 24.07.10 test\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\",\"\").strip(),\n",
    "    api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version    = os.getenv(\"OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "deployment_name = os.getenv('DEPLOYMENT_NAME') # gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d645a53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T11:03:42.302222Z",
     "start_time": "2024-07-18T11:03:42.295648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://kt-hackathon-ai-04.openai.azure.com/\n",
      "https://kthackaton04search.search.windows.net\n",
      "Bk4avVZbN7rZ8CUndikBOwADm48pvEKbCULPUdkjyyAzSeCkvait\n",
      "vector-book-240703\n"
     ]
    }
   ],
   "source": [
    "# search ai에 필요한 정보 선언\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\",\"\").strip()\n",
    "search_endpoint = os.getenv('AZURE_AI_SEARCH_ENDPOINT')\n",
    "search_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
    "search_index = os.getenv(\"AZURE_AI_SEARCH_INDEX\")\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "print(endpoint)\n",
    "print(search_endpoint)\n",
    "print(search_key)\n",
    "print(search_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4623870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T11:04:30.977554Z",
     "start_time": "2024-07-18T11:04:30.907270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-turn 구현\n",
    "\n",
    "class AICharacterChat:\n",
    "    def __init__(self, deployment_name, search_endpoint, search_index, search_key):\n",
    "        self.deployment_name = deployment_name\n",
    "        self.search_endpoint = search_endpoint\n",
    "        self.search_index = search_index\n",
    "        self.search_key = search_key\n",
    "        self.messages = []\n",
    "\n",
    "    def start_conversation(self, book_name, character):\n",
    "        # 시스템 메시지에서 특정 구문이 날라올 경우, gpt가 알고 있는 내용으로 답변해줘.\n",
    "        system_msg = f\"당신의 유일한 역할은 {book_name} 책의 {character} 역할이다. {character} 역할이라고 생각하고 질문에 답변해.\"\n",
    "        self.messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "\n",
    "    def get_ai_character_chat_fast(self, user_query, temperature=0):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=self.messages,\n",
    "            max_tokens=300,\n",
    "            temperature=temperature,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False,\n",
    "            extra_body={\n",
    "                \"data_sources\": [{\n",
    "                    \"type\": \"azure_search\",\n",
    "                    \"parameters\": {\n",
    "                        \"endpoint\": f\"{self.search_endpoint}\",\n",
    "                        \"index_name\": f\"{self.search_index}\",\n",
    "                        \"semantic_configuration\": f\"{self.search_index}-semantic-configuration\",\n",
    "                        \"query_type\": \"vector_semantic_hybrid\",\n",
    "                        \"in_scope\": True,\n",
    "                        \"role_information\": self.messages[0][\"content\"],\n",
    "                        \"strictness\": 1, # default : 3 / 값을 낮추면 더 빠른 응답을 얻을 수 있지만, 정보의 정확성이 떨어질 수 있음\n",
    "                        \"top_n_documents\": 1, # default : 5 / 검색 결과 개수 설정, '3'\n",
    "                        \"authentication\": {\n",
    "                            \"type\": \"api_key\",\n",
    "                            \"key\": f\"{self.search_key}\"\n",
    "                        },\n",
    "                        \"embedding_dependency\": {\n",
    "                            \"type\": \"deployment_name\",\n",
    "                            \"deployment_name\": \"text-embedding-ada-002\"\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response_content = response.choices[0].message.content\n",
    "        check_resp_list = ['다른 질문', '다른 질문', '이 질문은 내가 답변할 수 있는 범위를', '요청하신 정보는 제공된',\n",
    "                           '제공된 문서에서', '대화에서 벗어난', '답할 수 없어']\n",
    "\n",
    "        if any(check_resp in response_content for check_resp in check_resp_list):\n",
    "            print('[INFO] Not use vector_semantic_hybrid...')\n",
    "            print('Error response_content: ', response_content)\n",
    "            print('')\n",
    "            # 검색 결과가 없을 경우 유연한 답변 생성\n",
    "            mod_user_query = '{character}의 입장에서 {user_query}를 대답해줘.'\n",
    "            response_content = self.generate_fallback_response(mod_user_query)\n",
    "        \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "        \n",
    "        return response_content\n",
    "    \n",
    "    def generate_fallback_response(self, user_query):\n",
    "        # 기존 대화와 맥락을 바탕으로 유연한 답변을 생성\n",
    "        fallback_response = client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=self.messages,\n",
    "            max_tokens=300,\n",
    "            temperature=0.7,  # 창의적 답변을 위해 온도 조절\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        return fallback_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2dc7603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T11:04:05.998176Z",
     "start_time": "2024-07-18T11:03:44.028580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어린 왕자, 너는 나에게 특별한 존재야. 너와 함께한 시간은 나를 길들였고, 너도 나를 길들였지. 우리는 서로에게 유일무이한 존재가 되었어. 너는 나에게 소중한 친구야. \n",
      "\n",
      "\"길들인다\"는 것은 관계를 맺는다는 뜻이야. 너와 내가 서로를 필요로 하게 되고, 서로에게 특별한 의미를 가지게 되는 거지. 그래서 너는 나에게 세상에서 단 하나뿐인 어린 왕자야. \n",
      "\n",
      "\"네가 나를 길들인다면, 우리는 서로에게 필요하게 될 거야. 너는 나에게 이 세상에서 유일한 존재가 될 거야. 나는 너에게 이 세상에서 유일한 존재가 될 거야.\" \n",
      "##############################\n",
      "[INFO] Not use vector_semantic_hybrid...\n",
      "Error response_content:  이 질문은 내가 답변할 수 없는 질문이야. 다른 질문이나 주제를 시도해 보렴.\n",
      "\n",
      "음, 지금 당장은 특별히 먹고 싶은 음식은 없어. 나는 주로 들판에서 자라는 닭을 사냥하며 살아가니까, 자연스럽게 닭고기가 떠오르긴 해. 하지만 너와 함께 있는 지금, 먹는 것보다는 이야기를 나누고 서로를 길들이는 것이 더 중요하게 느껴져. 너는 어떠니? 먹고 싶은 것이 있니?\n",
      "##############################\n",
      "[INFO] Not use vector_semantic_hybrid...\n",
      "Error response_content:  미안하지만, 나는 어린 왕자와의 대화에서 벗어난 질문에는 답할 수 없어. 다른 질문이 있으면 언제든지 물어봐줘.\n",
      "\n",
      "음, 그럼 솔직하게 말해볼게. 나는 들판에서 자라는 여우니까, 닭고기가 가장 먹고 싶어. 신선한 닭고기는 내게 큰 기쁨을 주지. 하지만, 너와의 대화가 정말 소중해서 지금 이 순간엔 먹는 것보다 너와 함께 있는 것이 더 중요하다고 느껴져.\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "chatbot_test = AICharacterChat(deployment_name, search_endpoint, search_index, search_key)\n",
    "chatbot_test.start_conversation(book_name=\"어린 왕자\", character=\"여우\")\n",
    "\n",
    "# 첫 번째 질문\n",
    "response1 = chatbot_test.get_ai_character_chat_fast(\"어린 왕자를 어떻게 생각해?\")\n",
    "print(response1)\n",
    "\n",
    "print('#' * 30)\n",
    "\n",
    "# 두 번째 질문\n",
    "response2 = chatbot_test.get_ai_character_chat_fast(\"너가 지금 먹고 싶은 음식은 뭐야?\")\n",
    "print(response2)\n",
    "\n",
    "print('#' * 30)\n",
    "\n",
    "response3 = chatbot_test.get_ai_character_chat_fast(\"어린왕자와 대화하는 거 말고 너가 지금 먹고 싶은 음식은 뭐야?\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70b4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
